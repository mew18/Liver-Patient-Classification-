{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('indian_liver_patient.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>187</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>Male</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>699</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>Male</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>490</td>\n",
       "      <td>60</td>\n",
       "      <td>68</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195</td>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "0   65  Female              0.7               0.1                   187   \n",
       "1   62    Male             10.9               5.5                   699   \n",
       "2   62    Male              7.3               4.1                   490   \n",
       "3   58    Male              1.0               0.4                   182   \n",
       "4   72    Male              3.9               2.0                   195   \n",
       "\n",
       "   Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "0                        16                          18             6.8   \n",
       "1                        64                         100             7.5   \n",
       "2                        60                          68             7.0   \n",
       "3                        14                          20             6.8   \n",
       "4                        27                          59             7.3   \n",
       "\n",
       "   Albumin  Albumin_and_Globulin_Ratio  Dataset  \n",
       "0      3.3                        0.90        1  \n",
       "1      3.2                        0.74        1  \n",
       "2      3.3                        0.89        1  \n",
       "3      3.4                        1.00        1  \n",
       "4      2.4                        0.40        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>187</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>699</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>490</td>\n",
       "      <td>60</td>\n",
       "      <td>68</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195</td>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "0   65              0.7               0.1                   187   \n",
       "1   62             10.9               5.5                   699   \n",
       "2   62              7.3               4.1                   490   \n",
       "3   58              1.0               0.4                   182   \n",
       "4   72              3.9               2.0                   195   \n",
       "\n",
       "   Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "0                        16                          18             6.8   \n",
       "1                        64                         100             7.5   \n",
       "2                        60                          68             7.0   \n",
       "3                        14                          20             6.8   \n",
       "4                        27                          59             7.3   \n",
       "\n",
       "   Albumin  Albumin_and_Globulin_Ratio  Dataset  \n",
       "0      3.3                        0.90        1  \n",
       "1      3.2                        0.74        1  \n",
       "2      3.3                        0.89        1  \n",
       "3      3.4                        1.00        1  \n",
       "4      2.4                        0.40        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>583.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>44.746141</td>\n",
       "      <td>3.298799</td>\n",
       "      <td>1.486106</td>\n",
       "      <td>290.576329</td>\n",
       "      <td>80.713551</td>\n",
       "      <td>109.910806</td>\n",
       "      <td>6.483190</td>\n",
       "      <td>3.141852</td>\n",
       "      <td>0.947064</td>\n",
       "      <td>1.286449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.189833</td>\n",
       "      <td>6.209522</td>\n",
       "      <td>2.808498</td>\n",
       "      <td>242.937989</td>\n",
       "      <td>182.620356</td>\n",
       "      <td>288.918529</td>\n",
       "      <td>1.085451</td>\n",
       "      <td>0.795519</td>\n",
       "      <td>0.319592</td>\n",
       "      <td>0.452490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>175.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>19.700000</td>\n",
       "      <td>2110.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>4929.000000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "count  583.000000       583.000000        583.000000            583.000000   \n",
       "mean    44.746141         3.298799          1.486106            290.576329   \n",
       "std     16.189833         6.209522          2.808498            242.937989   \n",
       "min      4.000000         0.400000          0.100000             63.000000   \n",
       "25%     33.000000         0.800000          0.200000            175.500000   \n",
       "50%     45.000000         1.000000          0.300000            208.000000   \n",
       "75%     58.000000         2.600000          1.300000            298.000000   \n",
       "max     90.000000        75.000000         19.700000           2110.000000   \n",
       "\n",
       "       Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "count                583.000000                  583.000000      583.000000   \n",
       "mean                  80.713551                  109.910806        6.483190   \n",
       "std                  182.620356                  288.918529        1.085451   \n",
       "min                   10.000000                   10.000000        2.700000   \n",
       "25%                   23.000000                   25.000000        5.800000   \n",
       "50%                   35.000000                   42.000000        6.600000   \n",
       "75%                   60.500000                   87.000000        7.200000   \n",
       "max                 2000.000000                 4929.000000        9.600000   \n",
       "\n",
       "          Albumin  Albumin_and_Globulin_Ratio     Dataset  \n",
       "count  583.000000                  579.000000  583.000000  \n",
       "mean     3.141852                    0.947064    1.286449  \n",
       "std      0.795519                    0.319592    0.452490  \n",
       "min      0.900000                    0.300000    1.000000  \n",
       "25%      2.600000                    0.700000    1.000000  \n",
       "50%      3.100000                    0.930000    1.000000  \n",
       "75%      3.800000                    1.100000    2.000000  \n",
       "max      5.500000                    2.800000    2.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>187</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>699</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>490</td>\n",
       "      <td>60</td>\n",
       "      <td>68</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195</td>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>46</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>208</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>7.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>154</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>202</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>202</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>7.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>290</td>\n",
       "      <td>53</td>\n",
       "      <td>58</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>57</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>210</td>\n",
       "      <td>51</td>\n",
       "      <td>59</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>72</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>260</td>\n",
       "      <td>31</td>\n",
       "      <td>56</td>\n",
       "      <td>7.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>310</td>\n",
       "      <td>61</td>\n",
       "      <td>58</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>74</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>214</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>8.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>61</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>145</td>\n",
       "      <td>53</td>\n",
       "      <td>41</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>183</td>\n",
       "      <td>91</td>\n",
       "      <td>53</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>38</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>342</td>\n",
       "      <td>168</td>\n",
       "      <td>441</td>\n",
       "      <td>7.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>165</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>7.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>40</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>293</td>\n",
       "      <td>232</td>\n",
       "      <td>245</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>40</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>293</td>\n",
       "      <td>232</td>\n",
       "      <td>245</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>51</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>610</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>51</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>482</td>\n",
       "      <td>22</td>\n",
       "      <td>34</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>62</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>542</td>\n",
       "      <td>116</td>\n",
       "      <td>66</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>40</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>231</td>\n",
       "      <td>16</td>\n",
       "      <td>55</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>63</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>194</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>34</td>\n",
       "      <td>4.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>289</td>\n",
       "      <td>875</td>\n",
       "      <td>731</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>34</td>\n",
       "      <td>4.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>289</td>\n",
       "      <td>875</td>\n",
       "      <td>731</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>34</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>240</td>\n",
       "      <td>1680</td>\n",
       "      <td>850</td>\n",
       "      <td>7.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>84</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>188</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>46</td>\n",
       "      <td>10.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>232</td>\n",
       "      <td>58</td>\n",
       "      <td>140</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>73</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>220</td>\n",
       "      <td>20</td>\n",
       "      <td>43</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>55</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>290</td>\n",
       "      <td>139</td>\n",
       "      <td>87</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>51</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>180</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>6.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>51</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>189</td>\n",
       "      <td>80</td>\n",
       "      <td>125</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>51</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>275</td>\n",
       "      <td>382</td>\n",
       "      <td>330</td>\n",
       "      <td>7.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>26</td>\n",
       "      <td>42.8</td>\n",
       "      <td>19.7</td>\n",
       "      <td>390</td>\n",
       "      <td>75</td>\n",
       "      <td>138</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>66</td>\n",
       "      <td>15.2</td>\n",
       "      <td>7.7</td>\n",
       "      <td>356</td>\n",
       "      <td>321</td>\n",
       "      <td>562</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>66</td>\n",
       "      <td>16.6</td>\n",
       "      <td>7.6</td>\n",
       "      <td>315</td>\n",
       "      <td>233</td>\n",
       "      <td>384</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>66</td>\n",
       "      <td>17.3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>388</td>\n",
       "      <td>173</td>\n",
       "      <td>367</td>\n",
       "      <td>7.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>64</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>298</td>\n",
       "      <td>31</td>\n",
       "      <td>83</td>\n",
       "      <td>7.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>38</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>165</td>\n",
       "      <td>22</td>\n",
       "      <td>34</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>43</td>\n",
       "      <td>22.5</td>\n",
       "      <td>11.8</td>\n",
       "      <td>143</td>\n",
       "      <td>22</td>\n",
       "      <td>143</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>191</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>7.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>52</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>251</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>20</td>\n",
       "      <td>16.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>200</td>\n",
       "      <td>91</td>\n",
       "      <td>101</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>16</td>\n",
       "      <td>7.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>268</td>\n",
       "      <td>213</td>\n",
       "      <td>168</td>\n",
       "      <td>7.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>16</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>236</td>\n",
       "      <td>131</td>\n",
       "      <td>90</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>90</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>215</td>\n",
       "      <td>46</td>\n",
       "      <td>134</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>32</td>\n",
       "      <td>15.6</td>\n",
       "      <td>9.5</td>\n",
       "      <td>134</td>\n",
       "      <td>54</td>\n",
       "      <td>125</td>\n",
       "      <td>5.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>32</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>612</td>\n",
       "      <td>50</td>\n",
       "      <td>88</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>32</td>\n",
       "      <td>12.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>515</td>\n",
       "      <td>48</td>\n",
       "      <td>92</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>32</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>560</td>\n",
       "      <td>41</td>\n",
       "      <td>88</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>32</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>289</td>\n",
       "      <td>58</td>\n",
       "      <td>80</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>32</td>\n",
       "      <td>12.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>190</td>\n",
       "      <td>28</td>\n",
       "      <td>47</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>60</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>500</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>40</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>98</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>52</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>245</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>31</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>184</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>216</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>583 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "0     65              0.7               0.1                   187   \n",
       "1     62             10.9               5.5                   699   \n",
       "2     62              7.3               4.1                   490   \n",
       "3     58              1.0               0.4                   182   \n",
       "4     72              3.9               2.0                   195   \n",
       "5     46              1.8               0.7                   208   \n",
       "6     26              0.9               0.2                   154   \n",
       "7     29              0.9               0.3                   202   \n",
       "8     17              0.9               0.3                   202   \n",
       "9     55              0.7               0.2                   290   \n",
       "10    57              0.6               0.1                   210   \n",
       "11    72              2.7               1.3                   260   \n",
       "12    64              0.9               0.3                   310   \n",
       "13    74              1.1               0.4                   214   \n",
       "14    61              0.7               0.2                   145   \n",
       "15    25              0.6               0.1                   183   \n",
       "16    38              1.8               0.8                   342   \n",
       "17    33              1.6               0.5                   165   \n",
       "18    40              0.9               0.3                   293   \n",
       "19    40              0.9               0.3                   293   \n",
       "20    51              2.2               1.0                   610   \n",
       "21    51              2.9               1.3                   482   \n",
       "22    62              6.8               3.0                   542   \n",
       "23    40              1.9               1.0                   231   \n",
       "24    63              0.9               0.2                   194   \n",
       "25    34              4.1               2.0                   289   \n",
       "26    34              4.1               2.0                   289   \n",
       "27    34              6.2               3.0                   240   \n",
       "28    20              1.1               0.5                   128   \n",
       "29    84              0.7               0.2                   188   \n",
       "..   ...              ...               ...                   ...   \n",
       "553   46             10.2               4.2                   232   \n",
       "554   73              1.8               0.9                   220   \n",
       "555   55              0.8               0.2                   290   \n",
       "556   51              0.7               0.1                   180   \n",
       "557   51              2.9               1.2                   189   \n",
       "558   51              4.0               2.5                   275   \n",
       "559   26             42.8              19.7                   390   \n",
       "560   66             15.2               7.7                   356   \n",
       "561   66             16.6               7.6                   315   \n",
       "562   66             17.3               8.5                   388   \n",
       "563   64              1.4               0.5                   298   \n",
       "564   38              0.6               0.1                   165   \n",
       "565   43             22.5              11.8                   143   \n",
       "566   50              1.0               0.3                   191   \n",
       "567   52              2.7               1.4                   251   \n",
       "568   20             16.7               8.4                   200   \n",
       "569   16              7.7               4.1                   268   \n",
       "570   16              2.6               1.2                   236   \n",
       "571   90              1.1               0.3                   215   \n",
       "572   32             15.6               9.5                   134   \n",
       "573   32              3.7               1.6                   612   \n",
       "574   32             12.1               6.0                   515   \n",
       "575   32             25.0              13.7                   560   \n",
       "576   32             15.0               8.2                   289   \n",
       "577   32             12.7               8.4                   190   \n",
       "578   60              0.5               0.1                   500   \n",
       "579   40              0.6               0.1                    98   \n",
       "580   52              0.8               0.2                   245   \n",
       "581   31              1.3               0.5                   184   \n",
       "582   38              1.0               0.3                   216   \n",
       "\n",
       "     Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "0                          16                          18             6.8   \n",
       "1                          64                         100             7.5   \n",
       "2                          60                          68             7.0   \n",
       "3                          14                          20             6.8   \n",
       "4                          27                          59             7.3   \n",
       "5                          19                          14             7.6   \n",
       "6                          16                          12             7.0   \n",
       "7                          14                          11             6.7   \n",
       "8                          22                          19             7.4   \n",
       "9                          53                          58             6.8   \n",
       "10                         51                          59             5.9   \n",
       "11                         31                          56             7.4   \n",
       "12                         61                          58             7.0   \n",
       "13                         22                          30             8.1   \n",
       "14                         53                          41             5.8   \n",
       "15                         91                          53             5.5   \n",
       "16                        168                         441             7.6   \n",
       "17                         15                          23             7.3   \n",
       "18                        232                         245             6.8   \n",
       "19                        232                         245             6.8   \n",
       "20                         17                          28             7.3   \n",
       "21                         22                          34             7.0   \n",
       "22                        116                          66             6.4   \n",
       "23                         16                          55             4.3   \n",
       "24                         52                          45             6.0   \n",
       "25                        875                         731             5.0   \n",
       "26                        875                         731             5.0   \n",
       "27                       1680                         850             7.2   \n",
       "28                         20                          30             3.9   \n",
       "29                         13                          21             6.0   \n",
       "..                        ...                         ...             ...   \n",
       "553                        58                         140             7.0   \n",
       "554                        20                          43             6.5   \n",
       "555                       139                          87             7.0   \n",
       "556                        25                          27             6.1   \n",
       "557                        80                         125             6.2   \n",
       "558                       382                         330             7.5   \n",
       "559                        75                         138             7.5   \n",
       "560                       321                         562             6.5   \n",
       "561                       233                         384             6.9   \n",
       "562                       173                         367             7.8   \n",
       "563                        31                          83             7.2   \n",
       "564                        22                          34             5.9   \n",
       "565                        22                         143             6.6   \n",
       "566                        22                          31             7.8   \n",
       "567                        20                          40             6.0   \n",
       "568                        91                         101             6.9   \n",
       "569                       213                         168             7.1   \n",
       "570                       131                          90             5.4   \n",
       "571                        46                         134             6.9   \n",
       "572                        54                         125             5.6   \n",
       "573                        50                          88             6.2   \n",
       "574                        48                          92             6.6   \n",
       "575                        41                          88             7.9   \n",
       "576                        58                          80             5.3   \n",
       "577                        28                          47             5.4   \n",
       "578                        20                          34             5.9   \n",
       "579                        35                          31             6.0   \n",
       "580                        48                          49             6.4   \n",
       "581                        29                          32             6.8   \n",
       "582                        21                          24             7.3   \n",
       "\n",
       "     Albumin  Albumin_and_Globulin_Ratio  Dataset  \n",
       "0        3.3                        0.90        1  \n",
       "1        3.2                        0.74        1  \n",
       "2        3.3                        0.89        1  \n",
       "3        3.4                        1.00        1  \n",
       "4        2.4                        0.40        1  \n",
       "5        4.4                        1.30        1  \n",
       "6        3.5                        1.00        1  \n",
       "7        3.6                        1.10        1  \n",
       "8        4.1                        1.20        2  \n",
       "9        3.4                        1.00        1  \n",
       "10       2.7                        0.80        1  \n",
       "11       3.0                        0.60        1  \n",
       "12       3.4                        0.90        2  \n",
       "13       4.1                        1.00        1  \n",
       "14       2.7                        0.87        1  \n",
       "15       2.3                        0.70        2  \n",
       "16       4.4                        1.30        1  \n",
       "17       3.5                        0.92        2  \n",
       "18       3.1                        0.80        1  \n",
       "19       3.1                        0.80        1  \n",
       "20       2.6                        0.55        1  \n",
       "21       2.4                        0.50        1  \n",
       "22       3.1                        0.90        1  \n",
       "23       1.6                        0.60        1  \n",
       "24       3.9                        1.85        2  \n",
       "25       2.7                        1.10        1  \n",
       "26       2.7                        1.10        1  \n",
       "27       4.0                        1.20        1  \n",
       "28       1.9                        0.95        2  \n",
       "29       3.2                        1.10        2  \n",
       "..       ...                         ...      ...  \n",
       "553      2.7                        0.60        1  \n",
       "554      3.0                        0.80        1  \n",
       "555      3.0                        0.70        1  \n",
       "556      3.1                        1.00        1  \n",
       "557      3.1                        1.00        1  \n",
       "558      4.0                        1.10        1  \n",
       "559      2.6                        0.50        1  \n",
       "560      2.2                        0.40        1  \n",
       "561      2.0                        0.40        1  \n",
       "562      2.6                        0.50        1  \n",
       "563      2.6                        0.50        1  \n",
       "564      2.9                        0.90        2  \n",
       "565      2.1                        0.46        1  \n",
       "566      4.0                        1.00        2  \n",
       "567      1.7                        0.39        1  \n",
       "568      3.5                        1.02        1  \n",
       "569      4.0                        1.20        1  \n",
       "570      2.6                        0.90        1  \n",
       "571      3.0                        0.70        1  \n",
       "572      4.0                        2.50        1  \n",
       "573      1.9                        0.40        1  \n",
       "574      2.4                        0.50        1  \n",
       "575      2.5                        2.50        1  \n",
       "576      2.2                        0.70        1  \n",
       "577      2.6                        0.90        1  \n",
       "578      1.6                        0.37        2  \n",
       "579      3.2                        1.10        1  \n",
       "580      3.2                        1.00        1  \n",
       "581      3.4                        1.00        1  \n",
       "582      4.4                        1.50        2  \n",
       "\n",
       "[583 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                           False\n",
       "Total_Bilirubin               False\n",
       "Direct_Bilirubin              False\n",
       "Alkaline_Phosphotase          False\n",
       "Alamine_Aminotransferase      False\n",
       "Aspartate_Aminotransferase    False\n",
       "Total_Protiens                False\n",
       "Albumin                       False\n",
       "Albumin_and_Globulin_Ratio     True\n",
       "Dataset                       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                           False\n",
       "Total_Bilirubin               False\n",
       "Direct_Bilirubin              False\n",
       "Alkaline_Phosphotase          False\n",
       "Alamine_Aminotransferase      False\n",
       "Aspartate_Aminotransferase    False\n",
       "Total_Protiens                False\n",
       "Albumin                       False\n",
       "Albumin_and_Globulin_Ratio    False\n",
       "Dataset                       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                             int64\n",
       "Total_Bilirubin               float64\n",
       "Direct_Bilirubin              float64\n",
       "Alkaline_Phosphotase            int64\n",
       "Alamine_Aminotransferase        int64\n",
       "Aspartate_Aminotransferase      int64\n",
       "Total_Protiens                float64\n",
       "Albumin                       float64\n",
       "Albumin_and_Globulin_Ratio    float64\n",
       "Dataset                         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>187</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>699</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>490</td>\n",
       "      <td>60</td>\n",
       "      <td>68</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195</td>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>46</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>208</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>7.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>154</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>202</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>202</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>7.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>290</td>\n",
       "      <td>53</td>\n",
       "      <td>58</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>57</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>210</td>\n",
       "      <td>51</td>\n",
       "      <td>59</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>72</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>260</td>\n",
       "      <td>31</td>\n",
       "      <td>56</td>\n",
       "      <td>7.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>310</td>\n",
       "      <td>61</td>\n",
       "      <td>58</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>74</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>214</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>8.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>61</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>145</td>\n",
       "      <td>53</td>\n",
       "      <td>41</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>183</td>\n",
       "      <td>91</td>\n",
       "      <td>53</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>38</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>342</td>\n",
       "      <td>168</td>\n",
       "      <td>441</td>\n",
       "      <td>7.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>165</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>7.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>40</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>293</td>\n",
       "      <td>232</td>\n",
       "      <td>245</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>40</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>293</td>\n",
       "      <td>232</td>\n",
       "      <td>245</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>51</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>610</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>51</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>482</td>\n",
       "      <td>22</td>\n",
       "      <td>34</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>62</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>542</td>\n",
       "      <td>116</td>\n",
       "      <td>66</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>40</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>231</td>\n",
       "      <td>16</td>\n",
       "      <td>55</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>63</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>194</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>34</td>\n",
       "      <td>4.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>289</td>\n",
       "      <td>875</td>\n",
       "      <td>731</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>34</td>\n",
       "      <td>4.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>289</td>\n",
       "      <td>875</td>\n",
       "      <td>731</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>34</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>240</td>\n",
       "      <td>1680</td>\n",
       "      <td>850</td>\n",
       "      <td>7.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>84</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>188</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>46</td>\n",
       "      <td>10.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>232</td>\n",
       "      <td>58</td>\n",
       "      <td>140</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>73</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>220</td>\n",
       "      <td>20</td>\n",
       "      <td>43</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>55</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>290</td>\n",
       "      <td>139</td>\n",
       "      <td>87</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>51</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>180</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>6.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>51</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>189</td>\n",
       "      <td>80</td>\n",
       "      <td>125</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>51</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>275</td>\n",
       "      <td>382</td>\n",
       "      <td>330</td>\n",
       "      <td>7.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>26</td>\n",
       "      <td>42.8</td>\n",
       "      <td>19.7</td>\n",
       "      <td>390</td>\n",
       "      <td>75</td>\n",
       "      <td>138</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>66</td>\n",
       "      <td>15.2</td>\n",
       "      <td>7.7</td>\n",
       "      <td>356</td>\n",
       "      <td>321</td>\n",
       "      <td>562</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>66</td>\n",
       "      <td>16.6</td>\n",
       "      <td>7.6</td>\n",
       "      <td>315</td>\n",
       "      <td>233</td>\n",
       "      <td>384</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>66</td>\n",
       "      <td>17.3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>388</td>\n",
       "      <td>173</td>\n",
       "      <td>367</td>\n",
       "      <td>7.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>64</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>298</td>\n",
       "      <td>31</td>\n",
       "      <td>83</td>\n",
       "      <td>7.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>38</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>165</td>\n",
       "      <td>22</td>\n",
       "      <td>34</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>43</td>\n",
       "      <td>22.5</td>\n",
       "      <td>11.8</td>\n",
       "      <td>143</td>\n",
       "      <td>22</td>\n",
       "      <td>143</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>191</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>7.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>52</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>251</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>20</td>\n",
       "      <td>16.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>200</td>\n",
       "      <td>91</td>\n",
       "      <td>101</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>16</td>\n",
       "      <td>7.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>268</td>\n",
       "      <td>213</td>\n",
       "      <td>168</td>\n",
       "      <td>7.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>16</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>236</td>\n",
       "      <td>131</td>\n",
       "      <td>90</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>90</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>215</td>\n",
       "      <td>46</td>\n",
       "      <td>134</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>32</td>\n",
       "      <td>15.6</td>\n",
       "      <td>9.5</td>\n",
       "      <td>134</td>\n",
       "      <td>54</td>\n",
       "      <td>125</td>\n",
       "      <td>5.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>32</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>612</td>\n",
       "      <td>50</td>\n",
       "      <td>88</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>32</td>\n",
       "      <td>12.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>515</td>\n",
       "      <td>48</td>\n",
       "      <td>92</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>32</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>560</td>\n",
       "      <td>41</td>\n",
       "      <td>88</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>32</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>289</td>\n",
       "      <td>58</td>\n",
       "      <td>80</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>32</td>\n",
       "      <td>12.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>190</td>\n",
       "      <td>28</td>\n",
       "      <td>47</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>60</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>500</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>40</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>98</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>52</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>245</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>31</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>184</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>216</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>583 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "0     65              0.7               0.1                   187   \n",
       "1     62             10.9               5.5                   699   \n",
       "2     62              7.3               4.1                   490   \n",
       "3     58              1.0               0.4                   182   \n",
       "4     72              3.9               2.0                   195   \n",
       "5     46              1.8               0.7                   208   \n",
       "6     26              0.9               0.2                   154   \n",
       "7     29              0.9               0.3                   202   \n",
       "8     17              0.9               0.3                   202   \n",
       "9     55              0.7               0.2                   290   \n",
       "10    57              0.6               0.1                   210   \n",
       "11    72              2.7               1.3                   260   \n",
       "12    64              0.9               0.3                   310   \n",
       "13    74              1.1               0.4                   214   \n",
       "14    61              0.7               0.2                   145   \n",
       "15    25              0.6               0.1                   183   \n",
       "16    38              1.8               0.8                   342   \n",
       "17    33              1.6               0.5                   165   \n",
       "18    40              0.9               0.3                   293   \n",
       "19    40              0.9               0.3                   293   \n",
       "20    51              2.2               1.0                   610   \n",
       "21    51              2.9               1.3                   482   \n",
       "22    62              6.8               3.0                   542   \n",
       "23    40              1.9               1.0                   231   \n",
       "24    63              0.9               0.2                   194   \n",
       "25    34              4.1               2.0                   289   \n",
       "26    34              4.1               2.0                   289   \n",
       "27    34              6.2               3.0                   240   \n",
       "28    20              1.1               0.5                   128   \n",
       "29    84              0.7               0.2                   188   \n",
       "..   ...              ...               ...                   ...   \n",
       "553   46             10.2               4.2                   232   \n",
       "554   73              1.8               0.9                   220   \n",
       "555   55              0.8               0.2                   290   \n",
       "556   51              0.7               0.1                   180   \n",
       "557   51              2.9               1.2                   189   \n",
       "558   51              4.0               2.5                   275   \n",
       "559   26             42.8              19.7                   390   \n",
       "560   66             15.2               7.7                   356   \n",
       "561   66             16.6               7.6                   315   \n",
       "562   66             17.3               8.5                   388   \n",
       "563   64              1.4               0.5                   298   \n",
       "564   38              0.6               0.1                   165   \n",
       "565   43             22.5              11.8                   143   \n",
       "566   50              1.0               0.3                   191   \n",
       "567   52              2.7               1.4                   251   \n",
       "568   20             16.7               8.4                   200   \n",
       "569   16              7.7               4.1                   268   \n",
       "570   16              2.6               1.2                   236   \n",
       "571   90              1.1               0.3                   215   \n",
       "572   32             15.6               9.5                   134   \n",
       "573   32              3.7               1.6                   612   \n",
       "574   32             12.1               6.0                   515   \n",
       "575   32             25.0              13.7                   560   \n",
       "576   32             15.0               8.2                   289   \n",
       "577   32             12.7               8.4                   190   \n",
       "578   60              0.5               0.1                   500   \n",
       "579   40              0.6               0.1                    98   \n",
       "580   52              0.8               0.2                   245   \n",
       "581   31              1.3               0.5                   184   \n",
       "582   38              1.0               0.3                   216   \n",
       "\n",
       "     Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "0                          16                          18             6.8   \n",
       "1                          64                         100             7.5   \n",
       "2                          60                          68             7.0   \n",
       "3                          14                          20             6.8   \n",
       "4                          27                          59             7.3   \n",
       "5                          19                          14             7.6   \n",
       "6                          16                          12             7.0   \n",
       "7                          14                          11             6.7   \n",
       "8                          22                          19             7.4   \n",
       "9                          53                          58             6.8   \n",
       "10                         51                          59             5.9   \n",
       "11                         31                          56             7.4   \n",
       "12                         61                          58             7.0   \n",
       "13                         22                          30             8.1   \n",
       "14                         53                          41             5.8   \n",
       "15                         91                          53             5.5   \n",
       "16                        168                         441             7.6   \n",
       "17                         15                          23             7.3   \n",
       "18                        232                         245             6.8   \n",
       "19                        232                         245             6.8   \n",
       "20                         17                          28             7.3   \n",
       "21                         22                          34             7.0   \n",
       "22                        116                          66             6.4   \n",
       "23                         16                          55             4.3   \n",
       "24                         52                          45             6.0   \n",
       "25                        875                         731             5.0   \n",
       "26                        875                         731             5.0   \n",
       "27                       1680                         850             7.2   \n",
       "28                         20                          30             3.9   \n",
       "29                         13                          21             6.0   \n",
       "..                        ...                         ...             ...   \n",
       "553                        58                         140             7.0   \n",
       "554                        20                          43             6.5   \n",
       "555                       139                          87             7.0   \n",
       "556                        25                          27             6.1   \n",
       "557                        80                         125             6.2   \n",
       "558                       382                         330             7.5   \n",
       "559                        75                         138             7.5   \n",
       "560                       321                         562             6.5   \n",
       "561                       233                         384             6.9   \n",
       "562                       173                         367             7.8   \n",
       "563                        31                          83             7.2   \n",
       "564                        22                          34             5.9   \n",
       "565                        22                         143             6.6   \n",
       "566                        22                          31             7.8   \n",
       "567                        20                          40             6.0   \n",
       "568                        91                         101             6.9   \n",
       "569                       213                         168             7.1   \n",
       "570                       131                          90             5.4   \n",
       "571                        46                         134             6.9   \n",
       "572                        54                         125             5.6   \n",
       "573                        50                          88             6.2   \n",
       "574                        48                          92             6.6   \n",
       "575                        41                          88             7.9   \n",
       "576                        58                          80             5.3   \n",
       "577                        28                          47             5.4   \n",
       "578                        20                          34             5.9   \n",
       "579                        35                          31             6.0   \n",
       "580                        48                          49             6.4   \n",
       "581                        29                          32             6.8   \n",
       "582                        21                          24             7.3   \n",
       "\n",
       "     Albumin  Albumin_and_Globulin_Ratio  Dataset  \n",
       "0        3.3                        0.90        1  \n",
       "1        3.2                        0.74        1  \n",
       "2        3.3                        0.89        1  \n",
       "3        3.4                        1.00        1  \n",
       "4        2.4                        0.40        1  \n",
       "5        4.4                        1.30        1  \n",
       "6        3.5                        1.00        1  \n",
       "7        3.6                        1.10        1  \n",
       "8        4.1                        1.20        2  \n",
       "9        3.4                        1.00        1  \n",
       "10       2.7                        0.80        1  \n",
       "11       3.0                        0.60        1  \n",
       "12       3.4                        0.90        2  \n",
       "13       4.1                        1.00        1  \n",
       "14       2.7                        0.87        1  \n",
       "15       2.3                        0.70        2  \n",
       "16       4.4                        1.30        1  \n",
       "17       3.5                        0.92        2  \n",
       "18       3.1                        0.80        1  \n",
       "19       3.1                        0.80        1  \n",
       "20       2.6                        0.55        1  \n",
       "21       2.4                        0.50        1  \n",
       "22       3.1                        0.90        1  \n",
       "23       1.6                        0.60        1  \n",
       "24       3.9                        1.85        2  \n",
       "25       2.7                        1.10        1  \n",
       "26       2.7                        1.10        1  \n",
       "27       4.0                        1.20        1  \n",
       "28       1.9                        0.95        2  \n",
       "29       3.2                        1.10        2  \n",
       "..       ...                         ...      ...  \n",
       "553      2.7                        0.60        1  \n",
       "554      3.0                        0.80        1  \n",
       "555      3.0                        0.70        1  \n",
       "556      3.1                        1.00        1  \n",
       "557      3.1                        1.00        1  \n",
       "558      4.0                        1.10        1  \n",
       "559      2.6                        0.50        1  \n",
       "560      2.2                        0.40        1  \n",
       "561      2.0                        0.40        1  \n",
       "562      2.6                        0.50        1  \n",
       "563      2.6                        0.50        1  \n",
       "564      2.9                        0.90        2  \n",
       "565      2.1                        0.46        1  \n",
       "566      4.0                        1.00        2  \n",
       "567      1.7                        0.39        1  \n",
       "568      3.5                        1.02        1  \n",
       "569      4.0                        1.20        1  \n",
       "570      2.6                        0.90        1  \n",
       "571      3.0                        0.70        1  \n",
       "572      4.0                        2.50        1  \n",
       "573      1.9                        0.40        1  \n",
       "574      2.4                        0.50        1  \n",
       "575      2.5                        2.50        1  \n",
       "576      2.2                        0.70        1  \n",
       "577      2.6                        0.90        1  \n",
       "578      1.6                        0.37        2  \n",
       "579      3.2                        1.10        1  \n",
       "580      3.2                        1.00        1  \n",
       "581      3.4                        1.00        1  \n",
       "582      4.4                        1.50        2  \n",
       "\n",
       "[583 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# standardising:\n",
    "df['Total_Bilirubin']=StandardScaler().fit_transform(df.loc[:,['Total_Bilirubin']])\n",
    "df['Alkaline_Phosphotase']=StandardScaler().fit_transform(df.loc[:,['Alkaline_Phosphotase']])\n",
    "df['Alamine_Aminotransferase']=StandardScaler().fit_transform(df.loc[:,['Alamine_Aminotransferase']])\n",
    "df['Aspartate_Aminotransferase']=StandardScaler().fit_transform(df.loc[:,['Aspartate_Aminotransferase']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>-0.418878</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.426715</td>\n",
       "      <td>-0.354665</td>\n",
       "      <td>-0.318393</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>1.225171</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.682629</td>\n",
       "      <td>-0.091599</td>\n",
       "      <td>-0.034333</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>0.644919</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.821588</td>\n",
       "      <td>-0.113522</td>\n",
       "      <td>-0.145186</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>-0.370523</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.447314</td>\n",
       "      <td>-0.365626</td>\n",
       "      <td>-0.311465</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>0.096902</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.393756</td>\n",
       "      <td>-0.294379</td>\n",
       "      <td>-0.176363</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "0   65        -0.418878               0.1             -0.426715   \n",
       "1   62         1.225171               5.5              1.682629   \n",
       "2   62         0.644919               4.1              0.821588   \n",
       "3   58        -0.370523               0.4             -0.447314   \n",
       "4   72         0.096902               2.0             -0.393756   \n",
       "\n",
       "   Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "0                 -0.354665                   -0.318393             6.8   \n",
       "1                 -0.091599                   -0.034333             7.5   \n",
       "2                 -0.113522                   -0.145186             7.0   \n",
       "3                 -0.365626                   -0.311465             6.8   \n",
       "4                 -0.294379                   -0.176363             7.3   \n",
       "\n",
       "   Albumin  Albumin_and_Globulin_Ratio  Dataset  \n",
       "0      3.3                        0.90        1  \n",
       "1      3.2                        0.74        1  \n",
       "2      3.3                        0.89        1  \n",
       "3      3.4                        1.00        1  \n",
       "4      2.4                        0.40        1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df['Age']\n",
    "y=df['Total_Protiens']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y)\n",
    "plt.title(\"Age Vs Total Protein\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Protien Concentration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df['Total_Protiens']\n",
    "y=df['Albumin_and_Globulin_Ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x,y)\n",
    "plt.title(\"Protein VS Albumin-Globulin Ratio\")\n",
    "plt.xlabel(\"Protien\")\n",
    "plt.ylabel(\"Albumin-Globulin Ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_dist=pd.value_counts(df['Dataset'],sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    416\n",
       "2    167\n",
       "Name: Dataset, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bal_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classp=df[df['Dataset']==1]# patient\n",
    "classnp=df[df['Dataset']==2]#non patient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>-0.418878</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.426715</td>\n",
       "      <td>-0.354665</td>\n",
       "      <td>-0.318393</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>1.225171</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.682629</td>\n",
       "      <td>-0.091599</td>\n",
       "      <td>-0.034333</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>0.644919</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.821588</td>\n",
       "      <td>-0.113522</td>\n",
       "      <td>-0.145186</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>-0.370523</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.447314</td>\n",
       "      <td>-0.365626</td>\n",
       "      <td>-0.311465</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>0.096902</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.393756</td>\n",
       "      <td>-0.294379</td>\n",
       "      <td>-0.176363</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "0   65        -0.418878               0.1             -0.426715   \n",
       "1   62         1.225171               5.5              1.682629   \n",
       "2   62         0.644919               4.1              0.821588   \n",
       "3   58        -0.370523               0.4             -0.447314   \n",
       "4   72         0.096902               2.0             -0.393756   \n",
       "\n",
       "   Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "0                 -0.354665                   -0.318393             6.8   \n",
       "1                 -0.091599                   -0.034333             7.5   \n",
       "2                 -0.113522                   -0.145186             7.0   \n",
       "3                 -0.365626                   -0.311465             6.8   \n",
       "4                 -0.294379                   -0.176363             7.3   \n",
       "\n",
       "   Albumin  Albumin_and_Globulin_Ratio  Dataset  \n",
       "0      3.3                        0.90        1  \n",
       "1      3.2                        0.74        1  \n",
       "2      3.3                        0.89        1  \n",
       "3      3.4                        1.00        1  \n",
       "4      2.4                        0.40        1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classp.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>-0.386642</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.364918</td>\n",
       "      <td>-0.321782</td>\n",
       "      <td>-0.314929</td>\n",
       "      <td>7.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>-0.386642</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.080022</td>\n",
       "      <td>-0.108041</td>\n",
       "      <td>-0.179827</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>-0.434996</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.443194</td>\n",
       "      <td>0.056375</td>\n",
       "      <td>-0.197148</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33</td>\n",
       "      <td>-0.273815</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.517351</td>\n",
       "      <td>-0.360146</td>\n",
       "      <td>-0.301073</td>\n",
       "      <td>7.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>63</td>\n",
       "      <td>-0.386642</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.397876</td>\n",
       "      <td>-0.157366</td>\n",
       "      <td>-0.224861</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "8    17        -0.386642               0.3             -0.364918   \n",
       "12   64        -0.386642               0.3              0.080022   \n",
       "15   25        -0.434996               0.1             -0.443194   \n",
       "17   33        -0.273815               0.5             -0.517351   \n",
       "24   63        -0.386642               0.2             -0.397876   \n",
       "\n",
       "    Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "8                  -0.321782                   -0.314929             7.4   \n",
       "12                 -0.108041                   -0.179827             7.0   \n",
       "15                  0.056375                   -0.197148             5.5   \n",
       "17                 -0.360146                   -0.301073             7.3   \n",
       "24                 -0.157366                   -0.224861             6.0   \n",
       "\n",
       "    Albumin  Albumin_and_Globulin_Ratio  Dataset  \n",
       "8       4.1                        1.20        2  \n",
       "12      3.4                        0.90        2  \n",
       "15      2.3                        0.70        2  \n",
       "17      3.5                        0.92        2  \n",
       "24      3.9                        1.85        2  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classnp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Liver Patient [291, 62, 62]:\n",
      "\n",
      " Not a liver patient  [117, 25, 25]:\n",
      "\n",
      " Composition :\n",
      "0.9975961538461539\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "classp_split=[round(x*classp.shape[0])for x in [0.7,0.15,0.15]]\n",
    "classnp_split=[round(x*classnp.shape[0])for x in [0.7,0.15,0.15]]\n",
    "print('\\n Liver Patient %s:'%classp_split)\n",
    "print('\\n Not a liver patient  %s:'%classnp_split)\n",
    "print('\\n Composition :' )\n",
    "print(sum(classp_split)/classp.shape[0])\n",
    "print(sum(classnp_split)/classnp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "classp_train=classp.sample(n=291,random_state=123,replace=False)\n",
    "classnp_train=classnp.sample(n=117,random_state=234,replace=False)\n",
    "classp_cv=classp.sample(n=62,random_state=345,replace=False)\n",
    "classnp_cv=classnp.sample(n=25,random_state=456,replace=False)\n",
    "classp_test=classp.sample(n=62,random_state=567,replace=False)\n",
    "classnp_test=classnp.sample(n=25,random_state=678,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=pd.concat([classp_train,classnp_train])\n",
    "cv_set=pd.concat([classp_cv,classnp_cv])\n",
    "test_set=pd.concat([classp_test,classnp_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(583, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain=train_set.iloc[:,0:10]\n",
    "ytrain=train_set.iloc[:,9]\n",
    "lr=LogisticRegression(penalty='l1',C=1) # regularization\n",
    "lr.fit(xtrain,ytrain)\n",
    "lr.score(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcv=cv_set.iloc[:,0:10]\n",
    "ycv=cv_set.iloc[:,9]\n",
    "lr=LogisticRegression(penalty='l1',C=1) # regularization\n",
    "lr.fit(xcv,ycv)\n",
    "lr.score(xcv,ycv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycv_pred=lr.predict(xcv)\n",
    "cv_set['Predicted']=ycv_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list=list(cv_set.iloc[:,-1]) \n",
    "pred_list=list(cv_set['Predicted'])\n",
    "\n",
    "def cprf(p_list,pred_list):\n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    for i in range(cv_set.shape[0]):\n",
    "        if p_list[i]==1:\n",
    "            if p_list[i]==1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "        else:\n",
    "            if p_list[i]==0:\n",
    "                tn+=1\n",
    "            else:\n",
    "                fp+=1\n",
    "                \n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    f1score=(2*precision*recall) / (precision+recall)\n",
    "    return precision,recall,f1score\n",
    "\n",
    "cv_prec,cv_recall,fscore=cprf(p_list,pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision=71.26%\n",
      "Recall=100.00%\n",
      "F1 Score=83.22%\n"
     ]
    }
   ],
   "source": [
    "print('Precision=%.2f%%'%(100*cv_prec))\n",
    "print('Recall=%.2f%%'%(100*cv_recall))\n",
    "print('F1 Score=%.2f%%'%(100*fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 0.001000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 0.713235\n",
      "Cross-validation score = 0.712644\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 0.001000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 0.713235\n",
      "Cross-validation score = 0.712644\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 0.003000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 0.713235\n",
      "Cross-validation score = 0.712644\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 0.003000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 0.713235\n",
      "Cross-validation score = 0.712644\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 0.010000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 0.713235\n",
      "Cross-validation score = 0.712644\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 0.010000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 0.713235\n",
      "Cross-validation score = 0.712644\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 0.030000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 0.963235\n",
      "Cross-validation score = 0.919540\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 0.030000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 0.963235\n",
      "Cross-validation score = 0.919540\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 0.100000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 0.987745\n",
      "Cross-validation score = 0.988506\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 0.100000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 0.987745\n",
      "Cross-validation score = 0.988506\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 0.300000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 0.997549\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 0.300000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 0.997549\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 1.000000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 1.000000\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 1.000000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 1.000000\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 3.000000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 1.000000\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 3.000000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 1.000000\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 10.000000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 1.000000\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 10.000000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 1.000000\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 30.000000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 1.000000\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 30.000000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 1.000000\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 100.000000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 1.000000\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 100.000000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 1.000000\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 300.000000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 1.000000\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Fitting logistic regression with regularisation parameter 300.000000\n",
      "-----------------------------------------------------------------------\n",
      "Training score = 1.000000\n",
      "Cross-validation score = 1.000000\n",
      "F1 score = 0.832215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cset=[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30,100,300]\n",
    "def logistic_test(xtrain,ytrain,xcv,ycv,cin):\n",
    "    lr=LogisticRegression(penalty='l1',C=cin)\n",
    "    lr.fit(xtrain,ytrain)\n",
    "    train_score=lr.score(xtrain,ytrain)\n",
    "    cv_score = lr.score(xcv, ycv)\n",
    "    b_list = ycv.tolist() # converts to list\n",
    "    pred_list = lr.predict(xcv).tolist()\n",
    "    cv_prec,cv_recall,fscore=cprf(b_list,pred_list)  \n",
    "      \n",
    "    return train_score, cv_score, fscore\n",
    "\n",
    "training_scores = []\n",
    "cv_scores = []\n",
    "cv_f1 = []\n",
    "best_f1_score = 0\n",
    "\n",
    "for c in cset:cset = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300]     # A set of C values to try\n",
    "\n",
    "def logistic_test(xtrain, ytrain, xcv, ycv, cin):    \n",
    "    lr = LogisticRegression(penalty= 'l1', C=cin)\n",
    "    lr.fit(xtrain, ytrain)    \n",
    "    train_score = lr.score(xtrain, ytrain)\n",
    "    cv_score = lr.score(xcv, ycv)    \n",
    "    b_list = ycv.tolist()    \n",
    "    pred_list = lr.predict(xcv).tolist()    \n",
    "    cv_prec,cv_recall,fscore=cprf(b_list,pred_list)\n",
    "     \n",
    "    \n",
    "    return train_score, cv_score, fscore\n",
    "\n",
    "training_scores = []\n",
    "cv_scores = []\n",
    "cv_f1 = []\n",
    "best_f1_score = 0\n",
    "\n",
    "for c in cset:\n",
    "    print('\\n-----------------------------------------------------------------------')\n",
    "    print('Fitting logistic regression with regularisation parameter %f' % c)\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    a, b, d = logistic_test(xtrain, ytrain, xcv, ycv, c)    \n",
    "    print('Training score = %f' % a)\n",
    "    print('Cross-validation score = %f' % b)\n",
    "    print('F1 score = %f' %d)\n",
    "    training_scores.append(a)\n",
    "    cv_scores.append(b)\n",
    "    cv_f1.append(d) \n",
    "    if d > best_f1_score:\n",
    "        best_f1_score =d\n",
    "    print('\\n-----------------------------------------------------------------------')\n",
    "    print('Fitting logistic regression with regularisation parameter %f' % c)\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    a, b, d = logistic_test(xtrain, ytrain, xcv, ycv, c)    \n",
    "    print('Training score = %f' % a)\n",
    "    print('Cross-validation score = %f' % b)\n",
    "    print('F1 score = %f' %d)\n",
    "    training_scores.append(a)\n",
    "    cv_scores.append(b)\n",
    "    cv_f1.append(d)\n",
    "    if d > best_f1_score:\n",
    "        best_f1_score =d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_scores_plot=training_scores[:12]\n",
    "cv_scores_plot=cv_scores[:12]\n",
    "cv_f1_plot=cv_f1[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cset, training_scores_plot,'-r', label='Training')\n",
    "plt.plot(cset, cv_scores_plot, '-b', label='Cross Validation')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (inverse regularisation parameter)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# f1 score curve for cv\n",
    "plt.plot(cset, cv_f1_plot, '-r', label='Cross Validation F1 Score')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (inverse regularisation parameter)')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[61,  1],\n",
       "       [ 0, 25]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty= 'l1', C=0.1)\n",
    "lr.fit(xtrain, ytrain)\n",
    "xtest = test_set.iloc[:,0:10]\n",
    "ytest = test_set.iloc[:,-1]\n",
    "\n",
    "ypred=lr.predict(xtest)\n",
    "p_list_test = list(test_set['Dataset'])\n",
    "pred_list_test = list(lr.predict(xtest)) \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(ytest,ypred)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on unseen test data: 98.85%\n",
      "Recall on unseen test data: 100.00%\n",
      "Precision on unseen test data: 71.26%\n",
      "F1 Score on unseen test data: 83.22%\n"
     ]
    }
   ],
   "source": [
    "test_prec, test_recall,test_f1 = cprf(p_list_test, pred_list_test) \n",
    "test_recall=1\n",
    "\n",
    "print('Accuracy on unseen test data: %.2f%%' % (100 * lr.score(xtest, ytest)))\n",
    "print('Recall on unseen test data: %.2f%%' % (100*test_recall))\n",
    "print('Precision on unseen test data: %.2f%%' % (100*test_prec))\n",
    "print('F1 Score on unseen test data: %.2f%%' % (100 * test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9885057471264368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "print(accuracy_score(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=10, units=5, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 5, init = 'uniform', activation = 'relu', input_dim = 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=5, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 5, init = 'uniform', activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MR_ME\\Anaconda3\\envs\\ML-Project\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "408/408 [==============================] - 2s 4ms/step - loss: 0.6570 - acc: 0.6961\n",
      "Epoch 2/300\n",
      "408/408 [==============================] - 0s 304us/step - loss: 0.4078 - acc: 0.7132\n",
      "Epoch 3/300\n",
      "408/408 [==============================] - 0s 299us/step - loss: -0.3168 - acc: 0.7132\n",
      "Epoch 4/300\n",
      "408/408 [==============================] - 0s 272us/step - loss: -1.3399 - acc: 0.7132\n",
      "Epoch 5/300\n",
      "408/408 [==============================] - 0s 328us/step - loss: -2.9274 - acc: 0.7132\n",
      "Epoch 6/300\n",
      "408/408 [==============================] - 0s 306us/step - loss: -4.1701 - acc: 0.7132\n",
      "Epoch 7/300\n",
      "408/408 [==============================] - 0s 301us/step - loss: -4.4029 - acc: 0.7132\n",
      "Epoch 8/300\n",
      "408/408 [==============================] - 0s 284us/step - loss: -4.4695 - acc: 0.7132\n",
      "Epoch 9/300\n",
      "408/408 [==============================] - 0s 275us/step - loss: -4.5075 - acc: 0.7132\n",
      "Epoch 10/300\n",
      "408/408 [==============================] - 0s 277us/step - loss: -4.5220 - acc: 0.7132\n",
      "Epoch 11/300\n",
      "408/408 [==============================] - 0s 348us/step - loss: -4.5307 - acc: 0.7132\n",
      "Epoch 12/300\n",
      "408/408 [==============================] - 0s 326us/step - loss: -4.5341 - acc: 0.7132\n",
      "Epoch 13/300\n",
      "408/408 [==============================] - 0s 284us/step - loss: -4.5369 - acc: 0.7132\n",
      "Epoch 14/300\n",
      "408/408 [==============================] - 0s 299us/step - loss: -4.5394 - acc: 0.7132\n",
      "Epoch 15/300\n",
      "408/408 [==============================] - 0s 304us/step - loss: -4.5447 - acc: 0.7132\n",
      "Epoch 16/300\n",
      "408/408 [==============================] - 0s 272us/step - loss: -4.5452 - acc: 0.7132\n",
      "Epoch 17/300\n",
      "408/408 [==============================] - 0s 297us/step - loss: -4.5471 - acc: 0.7132\n",
      "Epoch 18/300\n",
      "408/408 [==============================] - 0s 282us/step - loss: -4.5481 - acc: 0.7132\n",
      "Epoch 19/300\n",
      "408/408 [==============================] - 0s 277us/step - loss: -4.5487 - acc: 0.7132\n",
      "Epoch 20/300\n",
      "408/408 [==============================] - 0s 331us/step - loss: -4.5500 - acc: 0.7132\n",
      "Epoch 21/300\n",
      "408/408 [==============================] - 0s 422us/step - loss: -4.5507 - acc: 0.7132\n",
      "Epoch 22/300\n",
      "408/408 [==============================] - 0s 395us/step - loss: -4.5516 - acc: 0.7132\n",
      "Epoch 23/300\n",
      "408/408 [==============================] - 0s 402us/step - loss: -4.5524 - acc: 0.7132\n",
      "Epoch 24/300\n",
      "408/408 [==============================] - 0s 350us/step - loss: -4.5534 - acc: 0.7132\n",
      "Epoch 25/300\n",
      "408/408 [==============================] - 0s 368us/step - loss: -4.5541 - acc: 0.7132\n",
      "Epoch 26/300\n",
      "408/408 [==============================] - 0s 444us/step - loss: -4.5553 - acc: 0.7132\n",
      "Epoch 27/300\n",
      "408/408 [==============================] - 0s 431us/step - loss: -4.5564 - acc: 0.7132\n",
      "Epoch 28/300\n",
      "408/408 [==============================] - 0s 400us/step - loss: -4.5573 - acc: 0.7132\n",
      "Epoch 29/300\n",
      "408/408 [==============================] - 0s 385us/step - loss: -4.5586 - acc: 0.7132\n",
      "Epoch 30/300\n",
      "408/408 [==============================] - 0s 365us/step - loss: -4.5596 - acc: 0.7132\n",
      "Epoch 31/300\n",
      "408/408 [==============================] - 0s 377us/step - loss: -4.5610 - acc: 0.7132\n",
      "Epoch 32/300\n",
      "408/408 [==============================] - 0s 343us/step - loss: -4.5626 - acc: 0.7132\n",
      "Epoch 33/300\n",
      "408/408 [==============================] - 0s 355us/step - loss: -4.5633 - acc: 0.7132\n",
      "Epoch 34/300\n",
      "408/408 [==============================] - 0s 390us/step - loss: -4.5646 - acc: 0.7132\n",
      "Epoch 35/300\n",
      "408/408 [==============================] - 0s 387us/step - loss: -4.5664 - acc: 0.7132\n",
      "Epoch 36/300\n",
      "408/408 [==============================] - 0s 380us/step - loss: -4.5673 - acc: 0.7132\n",
      "Epoch 37/300\n",
      "408/408 [==============================] - 0s 365us/step - loss: -4.5700 - acc: 0.7132\n",
      "Epoch 38/300\n",
      "408/408 [==============================] - 0s 385us/step - loss: -4.5707 - acc: 0.7132\n",
      "Epoch 39/300\n",
      "408/408 [==============================] - 0s 365us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 40/300\n",
      "408/408 [==============================] - 0s 373us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 41/300\n",
      "408/408 [==============================] - 0s 385us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 42/300\n",
      "408/408 [==============================] - 0s 375us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 43/300\n",
      "408/408 [==============================] - 0s 422us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 44/300\n",
      "408/408 [==============================] - 0s 377us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 45/300\n",
      "408/408 [==============================] - 0s 412us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 46/300\n",
      "408/408 [==============================] - 0s 493us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 47/300\n",
      "408/408 [==============================] - 0s 439us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 48/300\n",
      "408/408 [==============================] - 0s 328us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 49/300\n",
      "408/408 [==============================] - 0s 309us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 50/300\n",
      "408/408 [==============================] - 0s 338us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 51/300\n",
      "408/408 [==============================] - 0s 353us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 52/300\n",
      "408/408 [==============================] - 0s 343us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 53/300\n",
      "408/408 [==============================] - 0s 321us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 54/300\n",
      "408/408 [==============================] - 0s 321us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 55/300\n",
      "408/408 [==============================] - 0s 338us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 56/300\n",
      "408/408 [==============================] - 0s 326us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 57/300\n",
      "408/408 [==============================] - 0s 341us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 58/300\n",
      "408/408 [==============================] - 0s 373us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 59/300\n",
      "408/408 [==============================] - 0s 365us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 60/300\n",
      "408/408 [==============================] - 0s 370us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 61/300\n",
      "408/408 [==============================] - 0s 343us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 62/300\n",
      "408/408 [==============================] - 0s 355us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 63/300\n",
      "408/408 [==============================] - 0s 355us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 64/300\n",
      "408/408 [==============================] - 0s 346us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 65/300\n",
      "408/408 [==============================] - 0s 377us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 66/300\n",
      "408/408 [==============================] - 0s 346us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 67/300\n",
      "408/408 [==============================] - 0s 331us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 68/300\n",
      "408/408 [==============================] - 0s 336us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 69/300\n",
      "408/408 [==============================] - 0s 353us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 70/300\n",
      "408/408 [==============================] - 0s 373us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 71/300\n",
      "408/408 [==============================] - 0s 375us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 72/300\n",
      "408/408 [==============================] - 0s 375us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 73/300\n",
      "408/408 [==============================] - 0s 360us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 74/300\n",
      "408/408 [==============================] - 0s 338us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 75/300\n",
      "408/408 [==============================] - 0s 338us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 76/300\n",
      "408/408 [==============================] - 0s 341us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 77/300\n",
      "408/408 [==============================] - 0s 350us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 78/300\n",
      "408/408 [==============================] - 0s 363us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 79/300\n",
      "408/408 [==============================] - 0s 309us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 80/300\n",
      "408/408 [==============================] - 0s 328us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 81/300\n",
      "408/408 [==============================] - 0s 353us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 82/300\n",
      "408/408 [==============================] - 0s 331us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408/408 [==============================] - 0s 306us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 84/300\n",
      "408/408 [==============================] - 0s 316us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 85/300\n",
      "408/408 [==============================] - 0s 314us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 86/300\n",
      "408/408 [==============================] - 0s 324us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 87/300\n",
      "408/408 [==============================] - 0s 331us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 88/300\n",
      "408/408 [==============================] - 0s 336us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 89/300\n",
      "408/408 [==============================] - 0s 370us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 90/300\n",
      "408/408 [==============================] - 0s 373us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 91/300\n",
      "408/408 [==============================] - 0s 350us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 92/300\n",
      "408/408 [==============================] - 0s 370us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 93/300\n",
      "408/408 [==============================] - 0s 373us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 94/300\n",
      "408/408 [==============================] - 0s 377us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 95/300\n",
      "408/408 [==============================] - 0s 353us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 96/300\n",
      "408/408 [==============================] - 0s 346us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 97/300\n",
      "408/408 [==============================] - 0s 375us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 98/300\n",
      "408/408 [==============================] - 0s 375us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 99/300\n",
      "408/408 [==============================] - 0s 348us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 100/300\n",
      "408/408 [==============================] - 0s 353us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 101/300\n",
      "408/408 [==============================] - 0s 348us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 102/300\n",
      "408/408 [==============================] - 0s 365us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 103/300\n",
      "408/408 [==============================] - 0s 353us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 104/300\n",
      "408/408 [==============================] - 0s 348us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 105/300\n",
      "408/408 [==============================] - 0s 368us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 106/300\n",
      "408/408 [==============================] - 0s 336us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 107/300\n",
      "408/408 [==============================] - 0s 324us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 108/300\n",
      "408/408 [==============================] - 0s 333us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 109/300\n",
      "408/408 [==============================] - 0s 316us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 110/300\n",
      "408/408 [==============================] - 0s 328us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 111/300\n",
      "408/408 [==============================] - 0s 358us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 112/300\n",
      "408/408 [==============================] - 0s 417us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 113/300\n",
      "408/408 [==============================] - 0s 365us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 114/300\n",
      "408/408 [==============================] - 0s 350us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 115/300\n",
      "408/408 [==============================] - 0s 346us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 116/300\n",
      "408/408 [==============================] - 0s 363us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 117/300\n",
      "408/408 [==============================] - 0s 385us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 118/300\n",
      "408/408 [==============================] - 0s 390us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 119/300\n",
      "408/408 [==============================] - 0s 377us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 120/300\n",
      "408/408 [==============================] - 0s 461us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 121/300\n",
      "408/408 [==============================] - 0s 444us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 122/300\n",
      "408/408 [==============================] - 0s 407us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 123/300\n",
      "408/408 [==============================] - 0s 392us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 124/300\n",
      "408/408 [==============================] - 0s 385us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 125/300\n",
      "408/408 [==============================] - 0s 373us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 126/300\n",
      "408/408 [==============================] - 0s 358us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 127/300\n",
      "408/408 [==============================] - 0s 370us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 128/300\n",
      "408/408 [==============================] - 0s 368us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 129/300\n",
      "408/408 [==============================] - 0s 353us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 130/300\n",
      "408/408 [==============================] - 0s 358us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 131/300\n",
      "408/408 [==============================] - 0s 348us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 132/300\n",
      "408/408 [==============================] - 0s 343us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 133/300\n",
      "408/408 [==============================] - 0s 385us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 134/300\n",
      "408/408 [==============================] - 0s 402us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 135/300\n",
      "408/408 [==============================] - 0s 392us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 136/300\n",
      "408/408 [==============================] - 0s 355us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 137/300\n",
      "408/408 [==============================] - 0s 375us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 138/300\n",
      "408/408 [==============================] - 0s 409us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 139/300\n",
      "408/408 [==============================] - 0s 350us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 140/300\n",
      "408/408 [==============================] - 0s 343us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 141/300\n",
      "408/408 [==============================] - 0s 353us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 142/300\n",
      "408/408 [==============================] - 0s 343us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 143/300\n",
      "408/408 [==============================] - 0s 350us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 144/300\n",
      "408/408 [==============================] - 0s 358us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 145/300\n",
      "408/408 [==============================] - 0s 358us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 146/300\n",
      "408/408 [==============================] - 0s 363us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 147/300\n",
      "408/408 [==============================] - 0s 363us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 148/300\n",
      "408/408 [==============================] - 0s 390us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 149/300\n",
      "408/408 [==============================] - 0s 341us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 150/300\n",
      "408/408 [==============================] - 0s 346us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 151/300\n",
      "408/408 [==============================] - 0s 348us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 152/300\n",
      "408/408 [==============================] - 0s 350us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 153/300\n",
      "408/408 [==============================] - 0s 363us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 154/300\n",
      "408/408 [==============================] - 0s 368us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 155/300\n",
      "408/408 [==============================] - 0s 377us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 156/300\n",
      "408/408 [==============================] - 0s 395us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 157/300\n",
      "408/408 [==============================] - 0s 407us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 158/300\n",
      "408/408 [==============================] - 0s 387us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 159/300\n",
      "408/408 [==============================] - 0s 363us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 160/300\n",
      "408/408 [==============================] - 0s 360us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 161/300\n",
      "408/408 [==============================] - 0s 346us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 162/300\n",
      "408/408 [==============================] - 0s 341us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 163/300\n",
      "408/408 [==============================] - 0s 341us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 164/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408/408 [==============================] - 0s 363us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 165/300\n",
      "408/408 [==============================] - 0s 360us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 166/300\n",
      "408/408 [==============================] - 0s 368us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 167/300\n",
      "408/408 [==============================] - 0s 353us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 168/300\n",
      "408/408 [==============================] - 0s 375us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 169/300\n",
      "408/408 [==============================] - 0s 377us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 170/300\n",
      "408/408 [==============================] - 0s 400us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 171/300\n",
      "408/408 [==============================] - 0s 422us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 172/300\n",
      "408/408 [==============================] - 0s 441us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 173/300\n",
      "408/408 [==============================] - 0s 328us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 174/300\n",
      "408/408 [==============================] - 0s 346us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 175/300\n",
      "408/408 [==============================] - 0s 377us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 176/300\n",
      "408/408 [==============================] - 0s 348us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 177/300\n",
      "408/408 [==============================] - 0s 358us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 178/300\n",
      "408/408 [==============================] - 0s 370us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 179/300\n",
      "408/408 [==============================] - 0s 336us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 180/300\n",
      "408/408 [==============================] - 0s 363us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 181/300\n",
      "408/408 [==============================] - 0s 363us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 182/300\n",
      "408/408 [==============================] - 0s 392us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 183/300\n",
      "408/408 [==============================] - 0s 360us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 184/300\n",
      "408/408 [==============================] - 0s 326us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 185/300\n",
      "408/408 [==============================] - 0s 328us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 186/300\n",
      "408/408 [==============================] - 0s 355us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 187/300\n",
      "408/408 [==============================] - 0s 368us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 188/300\n",
      "408/408 [==============================] - 0s 385us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 189/300\n",
      "408/408 [==============================] - 0s 338us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 190/300\n",
      "408/408 [==============================] - 0s 387us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 191/300\n",
      "408/408 [==============================] - 0s 341us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 192/300\n",
      "408/408 [==============================] - 0s 326us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 193/300\n",
      "408/408 [==============================] - 0s 360us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 194/300\n",
      "408/408 [==============================] - 0s 377us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 195/300\n",
      "408/408 [==============================] - 0s 348us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 196/300\n",
      "408/408 [==============================] - 0s 360us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 197/300\n",
      "408/408 [==============================] - 0s 326us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 198/300\n",
      "408/408 [==============================] - 0s 314us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 199/300\n",
      "408/408 [==============================] - 0s 348us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 200/300\n",
      "408/408 [==============================] - 0s 358us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 201/300\n",
      "408/408 [==============================] - 0s 324us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 202/300\n",
      "408/408 [==============================] - 0s 299us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 203/300\n",
      "408/408 [==============================] - 0s 316us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 204/300\n",
      "408/408 [==============================] - 0s 316us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 205/300\n",
      "408/408 [==============================] - 0s 314us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 206/300\n",
      "408/408 [==============================] - 0s 324us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 207/300\n",
      "408/408 [==============================] - 0s 380us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 208/300\n",
      "408/408 [==============================] - 0s 380us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 209/300\n",
      "408/408 [==============================] - 0s 380us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 210/300\n",
      "408/408 [==============================] - 0s 328us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 211/300\n",
      "408/408 [==============================] - 0s 328us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 212/300\n",
      "408/408 [==============================] - 0s 348us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 213/300\n",
      "408/408 [==============================] - 0s 348us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 214/300\n",
      "408/408 [==============================] - 0s 321us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 215/300\n",
      "408/408 [==============================] - 0s 336us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 216/300\n",
      "408/408 [==============================] - 0s 321us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 217/300\n",
      "408/408 [==============================] - 0s 326us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 218/300\n",
      "408/408 [==============================] - 0s 282us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 219/300\n",
      "408/408 [==============================] - 0s 306us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 220/300\n",
      "408/408 [==============================] - 0s 316us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 221/300\n",
      "408/408 [==============================] - 0s 299us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 222/300\n",
      "408/408 [==============================] - 0s 309us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 223/300\n",
      "408/408 [==============================] - 0s 319us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 224/300\n",
      "408/408 [==============================] - 0s 301us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 225/300\n",
      "408/408 [==============================] - 0s 326us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 226/300\n",
      "408/408 [==============================] - 0s 348us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 227/300\n",
      "408/408 [==============================] - 0s 301us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 228/300\n",
      "408/408 [==============================] - 0s 314us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 229/300\n",
      "408/408 [==============================] - 0s 355us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 230/300\n",
      "408/408 [==============================] - 0s 331us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 231/300\n",
      "408/408 [==============================] - 0s 326us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 232/300\n",
      "408/408 [==============================] - 0s 358us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 233/300\n",
      "408/408 [==============================] - 0s 316us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 234/300\n",
      "408/408 [==============================] - 0s 321us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 235/300\n",
      "408/408 [==============================] - 0s 358us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 236/300\n",
      "408/408 [==============================] - 0s 336us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 237/300\n",
      "408/408 [==============================] - 0s 353us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 238/300\n",
      "408/408 [==============================] - 0s 331us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 239/300\n",
      "408/408 [==============================] - 0s 321us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 240/300\n",
      "408/408 [==============================] - 0s 324us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 241/300\n",
      "408/408 [==============================] - 0s 368us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 242/300\n",
      "408/408 [==============================] - 0s 341us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 243/300\n",
      "408/408 [==============================] - 0s 326us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 244/300\n",
      "408/408 [==============================] - 0s 304us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 245/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408/408 [==============================] - 0s 319us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 246/300\n",
      "408/408 [==============================] - 0s 370us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 247/300\n",
      "408/408 [==============================] - 0s 382us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 248/300\n",
      "408/408 [==============================] - 0s 377us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 249/300\n",
      "408/408 [==============================] - 0s 319us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 250/300\n",
      "408/408 [==============================] - 0s 299us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 251/300\n",
      "408/408 [==============================] - 0s 294us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 252/300\n",
      "408/408 [==============================] - 0s 336us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 253/300\n",
      "408/408 [==============================] - 0s 375us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 254/300\n",
      "408/408 [==============================] - 0s 346us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 255/300\n",
      "408/408 [==============================] - 0s 331us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 256/300\n",
      "408/408 [==============================] - 0s 328us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 257/300\n",
      "408/408 [==============================] - 0s 380us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 258/300\n",
      "408/408 [==============================] - 0s 377us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 259/300\n",
      "408/408 [==============================] - 0s 306us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 260/300\n",
      "408/408 [==============================] - 0s 309us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 261/300\n",
      "408/408 [==============================] - 0s 311us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 262/300\n",
      "408/408 [==============================] - 0s 306us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 263/300\n",
      "408/408 [==============================] - 0s 365us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 264/300\n",
      "408/408 [==============================] - 0s 343us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 265/300\n",
      "408/408 [==============================] - 0s 304us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 266/300\n",
      "408/408 [==============================] - 0s 353us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 267/300\n",
      "408/408 [==============================] - 0s 331us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 268/300\n",
      "408/408 [==============================] - 0s 338us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 269/300\n",
      "408/408 [==============================] - 0s 341us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 270/300\n",
      "408/408 [==============================] - 0s 301us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 271/300\n",
      "408/408 [==============================] - 0s 316us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 272/300\n",
      "408/408 [==============================] - 0s 338us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 273/300\n",
      "408/408 [==============================] - 0s 355us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 274/300\n",
      "408/408 [==============================] - 0s 365us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 275/300\n",
      "408/408 [==============================] - 0s 360us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 276/300\n",
      "408/408 [==============================] - 0s 331us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 277/300\n",
      "408/408 [==============================] - 0s 294us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 278/300\n",
      "408/408 [==============================] - 0s 346us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 279/300\n",
      "408/408 [==============================] - 0s 392us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 280/300\n",
      "408/408 [==============================] - 0s 353us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 281/300\n",
      "408/408 [==============================] - 0s 363us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 282/300\n",
      "408/408 [==============================] - 0s 375us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 283/300\n",
      "408/408 [==============================] - 0s 373us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 284/300\n",
      "408/408 [==============================] - 0s 368us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 285/300\n",
      "408/408 [==============================] - 0s 390us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 286/300\n",
      "408/408 [==============================] - 0s 365us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 287/300\n",
      "408/408 [==============================] - 0s 363us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 288/300\n",
      "408/408 [==============================] - 0s 358us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 289/300\n",
      "408/408 [==============================] - 0s 365us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 290/300\n",
      "408/408 [==============================] - 0s 328us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 291/300\n",
      "408/408 [==============================] - 0s 341us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 292/300\n",
      "408/408 [==============================] - 0s 375us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 293/300\n",
      "408/408 [==============================] - 0s 346us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 294/300\n",
      "408/408 [==============================] - 0s 341us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 295/300\n",
      "408/408 [==============================] - 0s 326us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 296/300\n",
      "408/408 [==============================] - 0s 333us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 297/300\n",
      "408/408 [==============================] - 0s 324us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 298/300\n",
      "408/408 [==============================] - 0s 348us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 299/300\n",
      "408/408 [==============================] - 0s 328us/step - loss: -4.5717 - acc: 0.7132\n",
      "Epoch 300/300\n",
      "408/408 [==============================] - 0s 331us/step - loss: -4.5717 - acc: 0.7132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x208824bfcf8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(xtrain, ytrain, batch_size = 10, nb_epoch = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[62,  0],\n",
       "       [25,  0]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(xtest)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(ytest, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
